# LAProp (Algorithm Implementation)

Recently I was working my way through a lot of deep learning books, and the subject of optimization came up, which I always found interesting. 
While looking for a different optimization algorithm (AMSProp), 
I stumbled upon a very interesting paper written by Liu Ziyin, Zhikang T.Wang, and Masahito Ueda called "LaProp: Separating Momentum and Adaptivity in Adam". 
Having implemented Adam from scratch earlier, I noticed the similarities and thought why not add this algorithm to the library that I was creating for myself. Fun exercise, and a very interesting paper!

Link to the paper: [LaProp: Separating Momentum and Adaptivity in Adam](https://arxiv.org/pdf/2002.04839.pdf)


### ToDo:

- add plots that compare performance of Adam and LAProp on existing datasets
- implementation in TensorFlow
